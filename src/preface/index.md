# Preface

AI is probably the biggest story of our lifetimes. Its rapid development, starting in the early 2020s, has generated a mixed outpouring of excitement, anxiety, and denial.

Everyone wants to weigh in. Even authors writing about history, language, biology, sociology, economics, the arts, and psychology feel the need to add a chapter about AI to the end of their books. And why not? AI will surely affect all of these far-flung domains, and many more—though exactly *how*, nobody can say with any confidence.

This book is a bit different. It’s less about the future of AI than it is about what we have learned in the present. At least as of 2024, few mainstream authors claim that AI is “real” intelligence.[^1] I do. Gemini, Claude, and ChatGPT aren’t powered by the same machinery as the human brain, of course, but intelligence is “multiply realizable,” meaning that, just as a computer algorithm can run on any kind of computer, intelligence can “run” on many physical substrates. In fact, although our brains aren’t much like the kinds of digital computers we have today, I think the substrate for intelligence *is* computation, which implies that a sufficiently powerful general-purpose computer can, by definition, implement intelligence. All it takes is the right code.

I will argue that, thanks to recent conceptual breakthroughs in AI development, we now know, at least at a high level, what that code *does*. We understand the essence of an incredibly powerful trick, although we’re still in the early days of making it work. Our implementations are neither complete, nor reliable, nor efficient—a bit like where we were with general computing when the ENIAC first powered up, in 1945, or where we were with aviation when the Wright brothers made their first powered flight, in 1903\.

It’s an old cliché in AI that airplanes and birds can both fly, but do so by different means.[^2] This truism has at times been used to motivate misguided approaches to AI. Still, the point stands. Birdflight is a biological marvel, which in some respects remains poorly understood, even today; and to engineer small drones, we still have much to learn from insects.[^3] However, we figured out the basic physics of flight—not *how* animals fly, but how it’s *possible* for them to—in the eighteenth century, with the discovery of Bernoulli’s Equation. Working airplanes took another century and a half to evolve.

Similarly, while there is still a great deal about the brain that we don’t understand, the fundamental principle behind intelligence—*prediction*—was first theorized by German polymath Hermann von Helmholtz in the 1860s.[^4] Many computational neuroscientists and AI researchers have elaborated on Helmholtz’s insight since then, and built limited models implementing aspects of the idea,[^5] but only recently has it become plausible to imagine that prediction really is the whole story. I will argue that, understood in full, the prediction principle may explain not only intelligence, but life itself.[^6]

Furthermore, it explains why recent large AI models really *are* intelligent; it’s not “anthropomorphic” to say so. This doesn’t mean that AI models are necessarily human-like, or lesser than us, or greater than us. In fact, understanding the curiously self-similar and self-referential nature of prediction will let us see that intelligence isn’t really a “thing.” It can’t exist in isolation, either in the three pounds of neural tissue in your head, or in the racks of computer chips running large models in datacenters. Intelligence is defined by networks, and by networks of networks. We can only understand what intelligence really is by changing how we think about it—by adopting a perspective that centers dynamic, symbiotic relationships rather than isolated minds.

This book takes on that project. Doing so requires weaving together insights from many different disciplines. As we go along, I’ll introduce concepts in probability, machine learning, physics, chemistry, biology, computer science, and other fields. When they’ve been most relevant to shaping our (sometimes mistaken) beliefs, I’ll also briefly review the intellectual histories of key ideas, from seventeenth century “mechanical philosophy” to debates about the origin of life, and from cybernetics to neuroscience.

You, dear reader, may be an expert in one or more of these fields, or in none. Few people are expert in all of them (I’m certainly not), so no specialized prior knowledge is assumed. On the other hand, even if you’re an AI researcher or neuroscientist with little patience for pop science, I hope you will find new and surprising ideas in this book. A general grasp of mathematical concepts like functions and variables will be helpful (with bonus points for knowing about vectors and matrices), but there will be no equations. (Well … *almost* none.) A general understanding of how computer programming works will be useful in a few places, but isn’t required. If you find understanding intelligence interesting enough to still be reading, rest assured: you are my audience.

The **Introduction** describes the recent emergence of “real” AI, sketching the hypothesis that prediction *is* intelligence, and pointing out some of the big questions that raises. The rest of the book consists of about a hundred bite-sized chapters that attempt to answer those questions, organized into the following ten parts:

1. **Origins** begins with “abiogenesis,” the emergence of life on Earth some four billion years ago. Darwin struggled to understand how life could have arisen. “Artificial life” experiments let us make more progress today, and suggest that life may be inevitable in a world capable of supporting computation (like ours). Understanding the deep role computation plays in our universe, and the way it links the seemingly disparate fields of physics, chemistry, and biology, both sheds new light on evolutionary theory and sets the stage for understanding the computational nature of intelligence.  
2. **Survival** considers the predictive intelligence hypothesis from the perspective of a tiny, simple organism capable of rudimentary computation: a bacterium. Being alive in the world is inherently social (even for bacteria), but this part imagines life as a one-player game, and considers what it takes to keep playing—which, since life is an infinite game, is the only prize there is.  
   **Interlude: The prehistory of computing** offers a historical perspective on the origins and founding assumptions of “traditional” computer science, from its roots in Enlightenment thinking and the Industrial Revolution to the development of the first electronic computers at the close of the Second World War.  
3. **Cybernetics** traces an alternative history of computing, which has recently re-emerged as modern AI, back to *its* foundations in the mid-twentieth century, describing how Norbert Wiener’s biologically-inspired theory of feedback lets us understand the connections between the predictive intelligence hypothesis and the development of artificial neural nets.  
4. **Learning** connects more recent advances in machine learning with our growing understanding of computational neuroscience and the evolution of nervous systems.  
5. **Other minds** explores how, as multiplayer life becomes complex, the main job of a mind becomes modeling other minds. This leads to a mutual modeling arms race, resulting in the kind of “intelligence explosion” that has produced humanity.  
6. **Many worlds** describes the way both long-term planning and short-term unpredictability (otherwise known as “free will”) emerge as consequences of social modeling, especially when applied to ourselves.  
7. **Ourselves** both explains and deconstructs consciousness in light of the above, asking: are we really as coherent a “self” as we believe? Answering will involve combining conceptual insights, experimental findings, and a high-level overview of what we know about the brain’s functional organization.  
8. **Transformer** chronicles the development of large language models and their multimodal successors. Do these AI models understand concepts? Are they intelligent? Can they reason?  
9. **Generality** explains both similarities and differences between Transformers and brains, shedding light on how Transformers achieve their generality and where gaps remain in their functionality. It also poses the most fraught question: do AI models have subjective experiences? Finally,  
10. **Evolutionary transition** zooms out and looks ahead. The emergence of AI probably won’t bring either rapture or apocalypse, but it does resemble earlier major symbiotic transitions on Earth, including those that led to eukaryotes, multicellular life, and photosynthesis. As in these earlier (and momentous) transitions, mutual prediction will generate unprecedented new levels of complexity and diversity. It will also require us to revise our political and economic assumptions, and even to rethink human identity.

The ideas in this book came together first gradually, over the course of decades as a software engineer and occasional computational neuroscientist, and then quickly, when my colleagues and I at Google found ourselves at the crest of AI’s “great wave” in the early 2020s.

Here’s my story, in brief. I’ve been obsessed with both brains and computers since childhood. In the late 1990s, as I was finishing a BA in Physics at Princeton, I began working with physicist and computational neuroscientist Bill Bialek on simulations to explore the relationship between what neurons actually do, which we have understood at a biophysical level since the 1950s, and what they compute.[^7] After I graduated, Bill asked if I would help set up the computers for a course he had been invited to co-direct that summer in Woods Hole, a small town on the elbow of Cape Cod. It was a wonderful opportunity to audit a course I wouldn’t otherwise have gotten into, and the experience proved life-changing.

The Marine Biological Laboratory in Woods Hole, founded in 1888, is at once ramshackle and illustrious. More than sixty Nobel Prize winners have been associated with it at one time or another. The summer courses are legendary, attracting visiting lecturers from all over the world. Old class photos from the course Bill had inherited, *Methods in Computational Neuroscience*, looked like a who’s who of the field twenty years later. They still do.

One of the postdocs in that 1998 cohort, Adrienne Fairhall, was, like Bill, a physicist who had decided to work on the brain. We’re now married, and a decade after we met at that course, *she* became its director\! Our kids spent many idyllic summers in Woods Hole, hanging out with the families of the other visiting scientists, rowing around in a little dinghy on Eel Pond and poking gingerly at bioluminescent comb jellies and other small creatures they scooped up in plastic buckets. Busy at their workbenches, the older researchers did much the same. Over the years, many organisms collected from those waters have provided fundamental insights into neural computation, from squids (whose giant axons were ideal for figuring out the biophysics of the action potential or “spike”), to the distributed nerve nets of jellyfish and *Hydra*, to the simple visual system of the Atlantic horseshoe crab, *Limulus polyphemus*.

While Adrienne established herself as a computational neuroscientist in the 2000s, I founded a tech startup. The startup was acquired by Microsoft, where I worked between 2006 and 2013\. One of my main areas of focus at the time was computer vision. Among other responsibilities, I led teams working on problems like reconstructing 3D scenes from images, tracking camera motion, and recognizing objects and text—all using hand-engineered or “Good Old Fashioned” AI (GOFAI). By the early 2010s, though, it was clear that the wind was shifting. Artificial Neural Nets (ANNs), which were much more explicitly brain-inspired than GOFAI algorithms, were taking off. The new approach was obviously the future.

So, at the end of 2013, I left Microsoft to join Google Research, the epicenter of this new convergence between AI and neuroscience. The following decade was a tremendously exciting time, during which I built many teams to work on a wide range of projects in AI. We did basic research in neural net-powered perception, media generation, AI bias and fairness, and privacy-preserving training algorithms. We also helped various product teams at Google build AI features and technologies, and hatched big dreams about what AI might ultimately make possible, beyond the envelope of any existing kind of product.

In late 2023, I founded a new research group at Google, Paradigms of Intelligence, to reimagine the foundations of AI. Our projects so far have included alternative, biologically inspired approaches to computation, new design ideas for efficient AI hardware, research into the social scaling of AI, and even artificial life.

I’ve been privileged to see and at times participate in the development of AI projects that were far enough ahead of the curve to seem like magic. Many of them caused me to rethink old assumptions, not only about AI, but also about the brain, intelligence, evolution, and the big philosophical questions that have dogged these topics for millennia. I began to write and give talks about some of these new insights, but synthesizing them into a coherent picture required more time and space than could fit into a lecture, essay, or paper.

This book is my first stab at bringing it all together. It’s an extended argument drawing on a wide range of work, from my own research group, from colleagues, and from much farther afield. In swinging for the fences, I’m sure to get some things wrong. I hope to keep revising and updating the material as AI advances at breakneck speed, and our understanding continues to improve. My hope and belief, though, is that the book’s core ideas trace a novel, enduring path through the complex territory of intelligence in all its forms.